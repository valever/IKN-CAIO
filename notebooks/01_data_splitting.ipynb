{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Splitting Strategy\n",
    "\n",
    "This notebook implements a comprehensive data splitting strategy for the fraud detection model. The strategy ensures robust model evaluation by properly separating data into distinct sets for training, validation, and testing.\n",
    "\n",
    "## üéØ Key Components\n",
    "\n",
    "1. **Out-of-Time (OOT) Split**\n",
    "   - Separates data based on transaction date\n",
    "   - Ensures temporal independence between training and test sets\n",
    "   - Helps evaluate model performance on future data\n",
    "\n",
    "2. **Train/Validation Split**\n",
    "   - Further splits training data into train and holdout sets\n",
    "   - Uses stratified sampling to maintain class distribution\n",
    "   - Enables model validation during training\n",
    "\n",
    "3. **Feature Transformation**\n",
    "   - Handles categorical variables\n",
    "   - Handles unknown categories\n",
    "   - Custom encodig enable control over categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries and Load Data <a name=\"import-libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data manipulation and model training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add project root to Python path for importing custom modules\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import custom encoder for handling categorical variables\n",
    "from utils.multicolumn_encoder import MultiColumnEncoder\n",
    "\n",
    "# Load processed data\n",
    "data = pd.read_parquet('../data/processed_credit_card_transactions.parquet')\n",
    "\n",
    "# set trannsaction num as index\n",
    "data.set_index('trans_num', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è∞ Out-of-Time Split\n",
    "\n",
    "Split data into training and out-of-time test sets based on transaction date.\n",
    "\n",
    "\n",
    "### Split Criteria\n",
    "- Training data: Transactions before July 2020\n",
    "- OOT data: Transactions from July 2020 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataframe based on expression \"trans_date_trans_time < '2020-07-01 00:00:00'\".\n"
     ]
    }
   ],
   "source": [
    "# Define split criteria based on transaction date\n",
    "_expression = \"trans_date_trans_time < '2020-07-01 00:00:00'\"\n",
    "print(f\"Splitting dataframe based on expression {_expression!r}.\")\n",
    "\n",
    "# Split into training and OOT sets\n",
    "train = data.query(_expression)\n",
    "oot = data.query(f\"~({_expression})\")\n",
    "\n",
    "# Separate features and target for OOT set\n",
    "oot_y = oot.is_fraud\n",
    "oot_X = oot.drop(columns=['is_fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Train/Validation Split\n",
    "\n",
    "Further split the training data into train and holdout sets using stratified sampling to maintain the class distribution. This helps in:\n",
    "\n",
    "1. Model validation during training\n",
    "2. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data by separating features and target\n",
    "X = train.drop(columns=['is_fraud'])\n",
    "y = train['is_fraud']\n",
    "\n",
    "# Split into train and holdout sets with stratification\n",
    "train_X, holdout_X, train_y, holdout_y = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # 20% of data for holdout\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Transform Features\n",
    "\n",
    "Encode categorical variables using our custom MultiColumnEncoder to prepare the data for model training. This ensures:\n",
    "\n",
    "1. Proper handling of categorical features\n",
    "2. Consistent encoding across all datasets\n",
    "3. Handling of unknown categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns in the dataset\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Initialize and fit encoder on training data\n",
    "encoder = MultiColumnEncoder(categorical_columns)\n",
    "train_X = encoder.fit_transform(train_X)\n",
    "\n",
    "# Transform holdout and OOT sets using fitted encoder\n",
    "holdout_X = encoder.transform(holdout_X)\n",
    "oot_X = encoder.transform(oot_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Split Datasets\n",
    "\n",
    "Save the processed datasets to parquet files for future use. This includes:\n",
    "\n",
    "1. Training data\n",
    "2. Holdout validation data\n",
    "3. Out-of-time test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and targets for each dataset\n",
    "train_data = pd.concat([train_X, train_y], axis=1)\n",
    "holdout_data = pd.concat([holdout_X, holdout_y], axis=1)\n",
    "oot_data = pd.concat([oot_X, oot_y], axis=1)\n",
    "\n",
    "# Save datasets to parquet files for future use\n",
    "train_data.to_parquet('../data/train_data.parquet')\n",
    "holdout_data.to_parquet('../data/holdout_data.parquet')\n",
    "oot_data.to_parquet('../data/oot_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
